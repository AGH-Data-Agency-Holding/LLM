# ğŸ¤– Client LLM â€” Recettes & Quran

## ğŸ“˜ AperÃ§u

Ce module `client_llm` implÃ©mente un **client intelligent** capable de communiquer :
- avec un **LLM local** (via `llama-cli` et le modÃ¨le *Mistral 7B*),
- avec un **backend distant FastAPI** (simulation locale),
- ou avec un **serveur LLM global** basÃ© sur RAG + Redis (centralisÃ©).

Lâ€™objectif est de fournir un systÃ¨me **hybride** et **rÃ©silient**, capable de fonctionner :
- ğŸ”¹ en **mode hors ligne (offline)** â€” via SQLite et le LLM local,
- ğŸ”¹ en **mode en ligne (online)** â€” via un backend FastAPI local,
- ğŸ”¹ en **mode serveur (server)** â€” via une API centralisÃ©e (RAG + cache Redis).

---

## ğŸ§± FonctionnalitÃ©s principales

### ğŸ³ Gestion des Recettes
- Base locale `recipes.db` initialisÃ©e Ã  partir de `data/recipes.json`
- Recherche rapide par ingrÃ©dient
- GÃ©nÃ©ration de suggestions via le LLM local ou distant

### ğŸ“– Gestion du Coran
- Base locale `surrah.db` crÃ©Ã©e depuis `data/quran_complete.json`
- Recherche par nom de sourate (arabe, franÃ§ais, anglais)
- Lecture de liens audio associÃ©s aux sourates

### ğŸ§  Modes de fonctionnement
| Mode | Source | Description |
|------|---------|-------------|
| **offline** | Local DB + LLM local | Fonctionne sans Internet |
| **online** | Backend FastAPI local | RequÃªtes simulÃ©es au serveur local |
| **server** | Serveur LLM RAG | Connexion Ã  lâ€™API centralisÃ©e avec cache Redis |

---

## ğŸ—‚ï¸ Structure du Projet
client_llm/
â”œâ”€â”€ init.py
â”œâ”€â”€ backend.py              # Backend FastAPI simulant un serveur distant
â”œâ”€â”€ llm_client.py           # Gestion du LLM local / distant / serveur global
â”œâ”€â”€ main_flow.py            # Flux principal (choix offline / online / server)
â”œâ”€â”€ local_db.py             # Initialisation et recherche dans les DB locales
â”œâ”€â”€ recipe_db           
â”œâ”€â”€ surrah_db        
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ recipes.json
â”‚   â”œâ”€â”€ quran_complete.json
â”‚   
â””â”€â”€ README.md

---

## âš™ï¸ Installation

### ğŸ§© PrÃ©requis

- Python â‰¥ 3.9
- FastAPI + Uvicorn
- ModÃ¨le Mistral tÃ©lÃ©chargÃ© pour `llama-cli`
- (Optionnel) Redis si tu veux tester le cache serveur global

### ğŸ§° Installation des dÃ©pendances
```bash
pip install fastapi uvicorn requests pydantic
python3 -m client_llm.main_flow
IngrÃ©dient Ã  rechercher : tomate
Mode (offline/online/server) : offline
RÃ©sultats locaux :
- Quiche tomates et Ã©pinards

Mode Online (Backend Local FastAPI)
uvicorn client_llm.backend:app --reload
python3 -m client_llm.main_flow
Mode (offline/online/server) : online
Aucune recette sur le serveur. GÃ©nÃ©ration LLM distant...
Recette gÃ©nÃ©rÃ©e par LLM distant pour 'Recette avec l'ingrÃ©dient tomate' (simulation)
Mode Server (RAG + Redis)
python3 -m client_llm.main_flow
Mode (offline/online/server) : server
[Cache Redis] La premiÃ¨re sourate du Coran est Al-Fatiha.
