# ğŸš€ LLM RAG & Caching API

## ğŸ“˜ Overview
Ce projet implÃ©mente une **API RESTful** basÃ©e sur **FastAPI** permettant de poser des questions Ã  un **LLM** en s'appuyant sur une architecture **RAG (Retrieval-Augmented Generation)** et un **cache Redis** pour rÃ©duire la latence. Plusieurs applications (Recettes, Coran, Qissas, etc.) coexistent de maniÃ¨re isolÃ©e grÃ¢ce Ã  un systÃ¨me de **multi-contextes** indexÃ© par `app_id`.

L'objectif est de fournir un service rapide, scalable et extensible, tout en assurant une sÃ©paration claire entre les donnÃ©es, la logique mÃ©tier et l'API.

---

## ğŸ§  FonctionnalitÃ©s Principales

### ğŸ” RAG (Retrieval-Augmented Generation)
* Utilisation d'embeddings (`multi-qa-mpnet-base-dot-v1`) pour retrouver les documents les plus pertinents.
* GÃ©nÃ©ration de rÃ©ponses contextualisÃ©es grÃ¢ce au modÃ¨le LLM (`mistral:7b`).
* DonnÃ©es RAG stockÃ©es dans `/data` (embeddings `.pt` et fichiers textuels `.json`).

### âš¡ Cache Intelligent (Redis)
* Les rÃ©ponses prÃ©cÃ©demment gÃ©nÃ©rÃ©es sont mises en cache.
* Le systÃ¨me rÃ©pond instantanÃ©ment si une rÃ©ponse est prÃ©sente dans Redis.
* RÃ©duction majeure du coÃ»t et du temps d'infÃ©rence.

### ğŸ§± Multi-Application
* Plusieurs jeux de donnÃ©es indÃ©pendants : Recette, Quran, Qissas, etc.
* Chaque application possÃ¨de ses propres embeddings et documents.
* Le `app_id` contrÃ´le la sÃ©lection du contexte.

---

## ğŸ—‚ï¸ Structure du Projet
```
.
â”œâ”€â”€ 0-data-processing/
â”œâ”€â”€ 1-cache/
â”œâ”€â”€ 2-isolation/
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ server_utils_optimised.py
â”‚   â”œâ”€â”€ documentation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ quran_embeddings.pt
â”‚   â”œâ”€â”€ quran_handling.json
â”‚   â”œâ”€â”€ recette_embeddings.pt
â”‚   â”œâ”€â”€ recette_handling.json
â”‚   â”œâ”€â”€ qissas_embeddings.pt
â”‚   â”œâ”€â”€ qissas_handling.json
â”œâ”€â”€ data_handling/
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration
Le fichier `2-isolation/config.py` centralise les paramÃ¨tres critiques.

### ParamÃ¨tres Principaux
| ParamÃ¨tre | Description | Valeur par dÃ©faut |
|----------|-------------|-------------------|
| `REDIS_HOST` | Adresse du serveur Redis | `localhost` |
| `REDIS_PORT` | Port Redis | `6379` |
| `OLLAMA_API_URL` | Endpoint Ollama | `http://localhost:11434/api/generate` |
| `OLLAMA_MODEL_NAME` | ModÃ¨le LLM | `mistral:7b` |
| `EMBEDDING_MODEL_NAME` | ModÃ¨le embeddings | `multi-qa-mpnet-base-dot-v1` |
| `SIMILARITY_THRESHOLD` | Seuil RAG | `0.85` |
| `APPLICATIONS_IDS` | Mapping app_id vers nom | Voir `config.py` |

### Mappings des Applications
```python
APPLICATIONS_IDS = {
    1234567890: "Application_Recette",
    1234567891: "Application_Quran",
    1234567892: "Application_Qissas"
}
```

---

## ğŸ§© Architecture Logique

### 1. Chargement des DonnÃ©es (RAG)
GÃ©rÃ© dans `load_application_data(app_id)` :
* Lecture des embeddings `.pt`.
* Lecture des documents `.json`.
* Mise en mÃ©moire RAM pour accÃ©lÃ©rer les recherches.

### 2. Trouver le Contexte Pertinent
* Embedding de la question.
* Comparaison avec les embeddings existants.
* Filtrage selon le seuil de similaritÃ©.

### 3. Appel LLM ou Cache
GÃ©rÃ© dans `ask_llm_with_redis_smart()` :
* VÃ©rifie si la rÃ©ponse existe dÃ©jÃ  dans Redis.
* Sinon, prÃ©pare un prompt enrichi.
* Appelle Ollama pour la gÃ©nÃ©ration.
* Stocke la rÃ©ponse dans Redis.

---

## ğŸŒ API REST
Le service expose un seul endpoint principal.

### `POST /ask`
Permet de poser une question dans un contexte donnÃ©.

#### Corps JSON
```json
{
  "app_id": 1234567891,
  "question": "Quel est le nom de la premiÃ¨re sourate du Coran ?"
}
```

#### RÃ©ponse Exemple
```json
{
  "response": "La premiÃ¨re sourate du Coran est Al-Fatiha.",
  "source": "LLM RAG"
}
```

`source` peut Ãªtre :
* `Cache Redis`
* `LLM RAG`

---

## â–¶ï¸ Lancer l'API

### PrÃ©requis
* Python 3.x
* Redis en local ou distant
* Ollama installÃ© avec `mistral:7b`

### Installation
```bash
pip install fastapi uvicorn pydantic python-dotenv redis
```

### DÃ©marrage
```bash
python 2-isolation/api.py
```
Serveur disponible sur : `http://0.0.0.0:8000`

---

## ğŸ“¦ Ajout de Nouvelles Applications RAG
1. Placer un nouveau fichier embeddings dans `/data`.
2. Placer son fichier textuel JSON dans `/data`.
3. Ajouter l'entrÃ©e dans `APPLICATIONS_IDS`.
4. RedÃ©marrer le serveur.

Le systÃ¨me est automatiquement extensible.

---

## âœ”ï¸ Avantages
* RÃ©ponses prÃ©cises et contextualisÃ©es.
* Latence minimale grÃ¢ce au caching.
* Architecture claire et maintenable.
* ScalabilitÃ© horizontale par application.

